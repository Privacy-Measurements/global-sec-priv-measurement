# ``Same Click, Different Risks'': Geography as a Hidden Factor in Web Privacy and Security

This repository contains the code and data-processing pipeline used to measure **client-side security and privacy behaviors of websites** using PageGraph instrumentation.

The repository includes the following modules:
- Construction of global, country-coded, and country-specific website catalogs.
- Crawling websites using a PageGraph-instrumented browser.
- Preprocessing crawl artifacts into structured databases.
- Large-scale analysis of tracking, fingerprinting, user identification.

## Repository Structure

### 1. `crux_urls/` — Website Catalog Construction

This directory contains all artifacts related to URL sourcing and catalog construction.


**Contents:**
- **`crawl_raw_urls/`**  
  Raw top-site lists collected from the Chrome UX Report (CrUX) snapshot dated **August 18**.

- **`suffixes/`**  
  Lists of country-code and geographic TLDs used to construct regional catalogs (e.g., `.de`, `.ae`, `.berlin`, `.dubai`).

- **`buckets/`**  
  Final website catalogs used in the study:
  - **D1**: Globally popular websites  
  - **D2**: Country-coded versions of global websites  
  - **D3**: Country-specific popular websites  

- **`urls_to_crawl/`**  
  URLs used for:
  - global catalog crawls,
  - VPN vs. physical vantage point ablation experiments.



### 2. `crawling/` — PageGraph-based Crawler

This directory contains the **Node.js project** responsible for crawling websites using a PageGraph-instrumented Brave browser.

#### Requirements
- Node.js and npm installed

#### Configuration

```bash
cd crawling
npm install
```

#### Running crawls


```bash
npm run pagegraph-crawl-using-given-urls "<PATH_TO_URL_FILE>"
```
---

### 3. `pre_processing/` — Crawl Artifact Processing

This module converts raw PageGraph outputs into structured formats suitable for analysis.

```
pre_processing/
├── process_graphml/    # Converts .graphml → structured JSON
└── process_database/   # Inserts processed data into SQL database
```

#### Components

- **`process_graphml/`**
  - Parses `.graphml` files generated by PageGraph
  - Outputs JSON files

- **`process_database/`**
  - Reads processed JSON files
  - Inserts data into a relational SQL database
   
---


### 4. `analysis/` — Security & Privacy Analysis

This directory contains **Python analysis scripts** that operate over the populated database.

```
analysis/
├── tracking/
├── fingerprinting/
├── user_identification/
```

---

## System Requirements


- **Node.js**: v20+
- **npm**: v10+
- **Python**: 3.9
- **Database**: MySQL or compatible SQL database
---

--

## Environment Configuration

The crawling and preprocessing pipeline requires environment configuration via a `.env` file.

### Copy .env_template into .env and update it

